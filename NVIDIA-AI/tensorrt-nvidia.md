# TensorRT-LLM 정리 #

  * TensorRT-LLM은 NVIDIA에서 개발한 오픈 소스 라이브러리
  * 대규모 언어 모델(LLM)의 추론 성능을 NVIDIA GPU에서 최적화하고 가속화하기 위해 설계되었음
  * 사용자 친화적인 Python API를 제공하여 LLM을 정의하고, 이를 효율적으로 실행할 수 있는 TensorRT 엔진으로 컴파일함.
  * 최신 최적화를 포함하여 NVIDIA GPU에서 효율적인 추론을 수행함.
  * Python 및 C++ 런타임을 포함해 TensorRT-LLM 엔진을 실행할 수 있는 컴포넌트 제공

### 1.  ###




### 공식 웹사이트 ###

* [NVIDIA TensorRT-LLM Documentation](https://docs.nvidia.com/tensorrt-llm/index.html)
* [NVIDIA TensorRT-LLM Github OpenSOurce](https://github.com/NVIDIA/TensorRT-LLM)

### 유투브 ###

* [Introducing TensorRT-LLM](https://youtu.be/hhhvZdkxsCE)


### 공식 블로그 ###
  
* [NVIDIA TensorRT-LLM 및 NVIDIA Triton Inference Server로 Meta Llama 3 성능 강화](https://developer.nvidia.com/ko-kr/blog/turbocharging-meta-llama-3-performance-with-nvidia-tensorrt-llm-and-nvidia-triton-inference-server/)
* [NVIDIA TensorRT-LLM, 인플라이트 배치로 인코더-디코더 모델 가속화](https://developer.nvidia.com/ko-kr/blog/nvidia-tensorrt-llm-now-accelerates-encoder-decoder-models-with-in-flight-batching/)
* [NVSwitch와 TensorRT-LLM 멀티샷으로 3배 빠른 AllReduce 구현](https://developer.nvidia.com/ko-kr/blog/3x-faster-allreduce-with-nvswitch-and-tensorrt-llm-multishot/)
* [추론 속도를 2배 높인 NVIDIA GH200 슈퍼칩, Llama 모델과의 멀티턴 상호작용에서 추론 가속화](https://developer.nvidia.com/ko-kr/blog/nvidia-gh200-superchip-accelerates-inference-by-2x-in-multiturn-interactions-with-llama-models/)
* [업그레이드된 NVIDIA TensorRT 10.0의 사용성, 성능, AI 모델 지원](https://developer.nvidia.com/ko-kr/blog/nvidia-tensorrt-10-0-upgrades-usability-performance-and-ai-model-support/)
* [Deploying a Large Language Model (LLM) with TensorRT-LLM on Triton Inference Server: A Step-by-Step Guide](https://medium.com/trendyol-tech/deploying-a-large-language-model-llm-with-tensorrt-llm-on-triton-inference-server-a-step-by-step-d53fccc856fa)
