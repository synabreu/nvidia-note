# 단일 GPU에서 효율적인 학습을 위한 방법 및 도구 #

이 문서는 모델 메모리 활용을 최적화하고 학습 속도를 높이거나 둘 다 향상시켜 모델 학습의 효율성을 높이는 실용적인 방법을 설명한다. 학습 과정에서 GPU가 어떻게 활용되는지 이해하고 싶다면, 먼저 [모델 학습 구조 개념 문서를](https://huggingface.co/docs/transformers/v4.48.2/en/model_memory_anatomy) 참고하라.

대규모 모델을 학습할 때는 다음 두 가지 측면을 동시에 고려해야 한다.

- 데이터 처리량/학습 시간  
- 모델 성능  

처리량(초당 샘플 수, Throughput)을 최대화하면 학습 비용을 절감할 수 있다. 일반적으로 이는 GPU를 최대한 활용하고 GPU 메모리를 한계까지 채우는 방식으로 이루어졌다. 만약 원하는 배치 크기가 GPU 메모리 한계를 초과하는 경우, 누적 그래디언트 업데이트(Gradient Accumulation Update)과 같은 메모리 최적화 기법을 도움이 될 수 있다. 

그러나 원하는 배치 크기가 메모리에 적합하게 들어간다면, 메모리 최적화 기법을 적용할 필요가 없다. 이러한 기법은 학습 속도를 저하시킬 수도 있기 때문에 단순히 큰 배치 크기를 사용할 수 있다고 해서 반드시 사용해야 하는 것은 아니다. 

하이퍼파라미터 튜닝의 일환으로, 어떤 배치 크기가 최상의 결과를 제공하는지 확인한 후, 이에 맞춰 자원을 최적화하는 것이 중요하다. 이 문서에서 다루는 방법과 도구는 학습 과정에 미치는 영향을 기준으로 아래와 같이 분류할 수 있다.

| 메서드/도구 | 훈련 속도 향상 | 메모리 활용 최적화 |
|----------------|----------------|---------------------|
| [배치 크기 선택](https://huggingface.co/docs/transformers/v4.48.2/en/perf_train_gpu_one#batch-size-choice) | 예             | 아니오               |
| [그래디언트 누적](https://huggingface.co/docs/transformers/v4.48.2/en/perf_train_gpu_one#gradient-accumulation) | 아니오         | 예                  |



### 용어 ###

1. [누적 그래디언트 업데이트(Gradient Accumulation Update)]() => 메모리 제한을 극복하기 위한 기법

   * 정의: GPU 메모리가 제한된 상황에서 큰 배치 크기를 효과적으로 처리하기 위한 기법, 메모리 제한 
   * 문제점 : 일반적으로 미니배치 크기를 늘리면 학습 안정성이 증가하지만, GPU 메모리 한계로 인해 대형 배치를 직접 사용할 수 없는 경우가 많다. 
   * 솔루션 : 여러 개의 작은 미니배치에서 계산한 그래디언트를 누적한 후, 일정 횟수마다 가중치를 업데이트하는 방식 적용
   * 예) 한 번에 32개의 샘플을 처리하고 싶지만 GPU 메모리 제한으로 인해 8개씩만 처리할 수 있다면, 4번의 미니배치에서 그래디언트를 모은 후 한 번의 업데이트를 수행함.
     
